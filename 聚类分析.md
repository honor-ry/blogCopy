---
title: 模式识别(二)--聚类分析
date: 2018-09-11 22:28:50
tags:  模式识别
categories: "模式识别"
mathjax: true

---

物以俱类，人以群分<!--more-->

## 2.1 距离聚类的概念

### 物以俱类

”物以俱类，人以群分“，反映了聚类分析的基本思想。

聚类分析属于非监督分类，即基本上无先验知识可依据或参考。

聚类分析根据模式之间的相似性对模式进行分类，对一批没有标出类别的模式样本集，将相似的归为一类，不相似的归为一类。

### 相似性

当研究一个复杂对象时，可以对其特征进行各种可能的测量，将测量值组成向量形式，称为该样本的特征向量，有$n$个特征值组成的就是$n$维向量，相当于空间中的一个点，整个模式样本集的特征向量可以看作分布在特征空间中的一些点。

可以将特征空间中点与点之间的距离函数作为模式相似性的测量，以“距离”作为模式分类的依据，距离越小，越相似。

## 2.2 相似度测量和距离准则

### 2.2.1 相似性测度

相似性测度是衡量模式之间相似性的一种尺度。距离就是一种相似性的测度，他可以用来度量同一类模式之间的类似性和不属于同一类的模式之间的差异性。下面介绍几种距离：

**1.欧式距离**

设$X_i,X_j$为两个$n$维模式，$X_i=(x_{i1},x_{i2}, \dots, x_{in})^T$，$X_j=(x_{j1},x_{j2}, \dots, x_{jn})^T$，则定义欧式距离为：

$D(X_i,X_j)=|||X_i-X_j||=\sqrt{(X_i-X_j)^T(X_i-X_j)}=\sqrt{(x_{i1}-x_{j1})^2+\dots+(x_{in}-x_{jn})^2}$

**2.马氏距离**

马氏距离常用平方形式表示。设$X$为模式向量，$M$为某类模式的均值向量，$C$为该类模式总体的协方差矩阵，则马氏距离定义为：

$D^2 = (X-M)^TC^{-1}(X-M)$

对$n$为向量可表示为：$X=(x_{1},x_{2}, \dots, x_{n})^T$,$M=(m_{1},m_{2}, \dots, m_{n})^T$,$C=E{(X-M)(X-M)^T}。

![](https://i.imgur.com/whme94d.png)

协方矩阵C表示的是各分量上模式样本到均值的距离，也就是在各维上模式的分散情况。

**3.明氏距离**

设$X_i, X_j$为$n$维模式向量，$X_i, X_j$之间的明氏距离$D_m$表示为：

$D_m(X_i,X_j)=[\sum_{k=1}^n|x_{ik}-x_{jk}|^m]^{\frac{1}{m}}$

当$m=2$时，明氏距离就是欧式距离。
当$m=1$时，可得：

$D_1(X_i,X_j)=\sum_{k=1}^n|x_{ik}-x_{jk}|$

称其为“街坊”距离。

![](https://i.imgur.com/VhVHTyV.png)

**4.汉明距离**

如果模式向量各分量的值仅取1或（-1），即二值模式，则可用汉民距离衡量模式间的相似性。设$X_i, X_j$为$n$维二值模式向量，$X_i, X_j$之间的汉明距离$D_m$表示为：

$D_h(X_i,X_j)=\frac{1}{2}(n-\sum_{k=1}^nx_{ik} \cdot x_{jk})$

由定义可知，若两个模式向量的每个分量取值都不同，则汉明距离为n；若两个模式向量的各分量取值都相同，则汉明距离为零。

**5.角度相似性函数**

角度相似性函数表示为：

![](https://i.imgur.com/Eguul2a.png)

它是模式之间的余弦。

### 2.2.2 聚类准测

聚类准则：根据相似性测度确定的，衡量模式之间是否相似的标准。即把不同模式聚为一类还是归为不同类的准则。

确定聚类准则的两种方式：

- 1.阈值准则：根据规定的距离阈值进行分类的准则。
- 2.函数准则：利用聚类准则函数进行分类的准则。

聚类准则函数：在聚类分析中，表示模式类间相似或差异性的函数。

一种常用的指标是误差平方之和，聚类准则函数定义如下：

$J = \sum_{j=1}^c \sum_{x \in S_j} ||X-M_j||^2$

其中，$c$表示共有$c$个模式类；$M_j = \frac{1}{N_j} \sum_{X \in S_j}X$,为$S_j$中模式样本的均值向量，$N_j$为$S_j$中的模式样本数量。

J代表了分属于c个聚类类别的全部模式样本与其相应类别模式均值之间的误差平方和。 

## 2.3 基于距离阈值的聚类算法

### 2.3.1 近邻聚类算法

#### 问题：

有N个待分类的模式$\lbrace X_1,X_2, \dots, X_N \rbrace$，要求按距离阈值T分类到以$Z_1, Z_2,\dots$为聚类中心的模式类中。

#### 算法描述

1、任取样本Xi作为第一个聚类中心的初始值，如令$Z_1 = X_1$。

2、算样本X2到Z1的欧式距离$D_{21}= ||X_2 - Z_1||$，若$D_{21}>T$，定义一新的聚类中心$Z_2 = X_2%；否则$X2 \in$以$Z_1$为中心的聚类。

3、假设已有聚类中心$Z_1,Z_2$，计算$D_{31}=||X_3 - Z_1||$和$D_{32}=||X_3 - Z_2||$，若$D_{31}>T$且$D_{32}>T$，则建立第三个聚类中心$Z_3 = X_3$；否则$X3 \in$离$Z_1$和$Z_2$中最近着（最近邻的聚类中心）。

……以此类推，直到将所有的N个样本都进行分类。

#### 算法特点

局限性：很大程度上依赖于第一个聚类中心的位置选择、待分类模式样本的排列次序、距离阈值T的大小以及样本分布的几何性质等。

优点：计算简单。（一种虽粗糙但快速的方法）

### 2.3.2 最大最小距离算法

#### 问题：

有N个待分类的模式$\lbrace X_1,X_2, \dots, X_N \rbrace$，要求按距离阈值T分类到以$Z_1, Z_2,\dots$为聚类中心的模式类中。

#### 算法描述

1、任取样本Xi作为第一个聚类中心的初始值，如令$Z_1 = X_1$。

2、选择离$Z_1$距离最远的模式样本作为第二个聚类中心$Z_2$。

3、逐个计算每个模式样本与已确定的所有聚类中心之间的距离，并选出其中最小距离。 


例当聚类中心数k=2时，计算

$D_{i1} = ||X_i-Z_1||$,  $D_{i2} = ||X_i-Z_2||$

并求出：

$\min{(D_{i1},D_{i2})}$

因为共有N个模式样本，所以此时得到N个最小距离。

4、在所有最小距离中选出一个最大距离，如果该最大值达到$||Z_1-Z_2||$的一定分数以上，则将产生最大距离的那个模式样本定义定义为**新增的聚类中心**，并返回上一步。否则，聚类中心的计算步骤结束。这里的“$||Z_1-Z_2||$的一定分数比值”就是阈值$T$,即

$T = \theta ||Z_1-Z_2||$

同样，即$k=2$为例，若$\max{\lbrace \min{(D_{i1},D_{i2})}, i=1,2,\dots, N \rbrace} > T$，则$Z_3$存在，并取为相应的模式向量，返回步骤三；否则寻找聚类中心结束。

5、重复步骤3和步骤4，直到没有聚类中心出现为止。

6、寻找聚类中心的运算结束后，将模式样本$\lbrace X_i, i =1,2,\dots,N \rbrace$按最近距离划分到相应聚类中心所代表的类别中。

## 2.4 层次聚类

层次聚类法也称为系统聚类法或分级聚类法。这种方法同样采用距离与之作为决策聚类数目的标准，基本思路：每个样本先自成一类，然后按距离准则逐步合并，减少类数，直到达到分类要求为止。

#### 算法描述

1、N个初始化模式样本自成一类，即建立N类$G_1(0),G_2(0),\dots,G_N(0)$。计算各类之间（各样本间）的距离，得到一个$N \times N$维的距离矩阵$D(0)$。标号$(0)$表示是聚类开始运算前的状态。

2、如在前一步聚类运算中，以求得距离矩阵$D(n)$(n为逐次聚类合并得次数)，则找出$D(n)$中得最小元素，将其对应得两类合并为一类。由此建立新的分类：$G_1(n+1),G_2(n+1),\dots$。

3、计算合并后新类别之间得距离，得到距离矩阵$D(n+1)$。

4、 跳至第2步，重复计算及合并。

结束条件：设定一个距离阈值$T$，当$D(n)$的最小值分量超过给定值$T$时，算法停止。

在第3步中，计算合并后的聚类与其他没有合并的模式类之间的距离，或者两个合并后的聚类之间的距离时，应如何计算？下面介绍几种不同的类间距离计算准则：

- （1）最短距离法
- （2）最长距离法
- （3）中间距离法
- （4）重心法
- （5）类平均距离法

## 2.5 动态聚类法

动态聚类法首先选择若干个模式样本作为聚类中心，再按照实现确定的聚类准则进行聚类。在聚类过程中，根据聚类准则对聚类中心进行反复修改，直到分类合理为止。

### 2.5.1 K-均值算法

K-均值算法又称为C-均值算法，时根据函数准则进行分类的聚类算法，基于使聚类准则函数最小化。

![](https://i.imgur.com/PVtenpJ.png)

#### 算法描述

设共有$N$个模式样本，计算步骤如下：

1、 任选$K$个初始化聚类中心$Z_1(1),Z_1(2),Z_K(1), K < N$。括号内的序号代表寻找聚类中心的迭代次序号。一般选择样本集中前$K$个样本作为初始聚类中心。

2、 按**最短距离原则**将其余样本分配到$K$个聚类中心中的某一个，即

若 $\min{\lbrace ||X-Z_i(k)||, i =1,2,\dots, K\rbrace} = ||X-Z_j(k)||=D_j(k)$
则，$X \in S_j(k)$

式中，$k$即为迭代运算的次序号。

3、计算各个聚类中心的新向量值$Z_j(k+1),j =1,2,\dots,K$

$Z_j(k+1) = \frac{1}{N_j} \sum_{X \in S_j(k)}X, j = 1,2,\dots, K$

即以均值向量作为新的聚类中心。这一步要分别计算$K$个聚类中的样本均值向量，故称算法为K-均值聚类。

4、如果$Z_j(k+1) \neq Z_j(k),j =1,2,\dots,K$，则回到步骤2，将模式样本逐个重新分类，并重复迭代计算，如果$Z_j(k+1) = Z_j(k),j =1,2,\dots,K$，算法收敛，计算完毕。

K均值聚类算法的思想是：首先随机选取K个对象作为初始的聚类中心，然后，计算每个对象距聚类中心的距离，把每个对象归到距离它最近的聚类中心。如果全部对象被分配完，则每个聚类的聚类中心将根据现有的对象重新计算。直到满足某个终止条件。K均值聚类算法是一种无监督学习算法。具体算法实现如下：

![](https://i.imgur.com/BHa0iXW.png)

## 代码

	import numpy as np
	import random
	import matplotlib.pyplot as plt
	
	#计算样本到中心的距离,采用欧式距离
	def distEC(data,center,k):
	    num = np.array(data)
	    hang = num.shape[0] #数据个数
	    lie = num.shape[1] #指标个数
	    dist = [[0]*k for row in range(hang)]
	    for i in range(hang):
	        for l in range(k):
	            s = 0
	            for j in range(lie):
	                s =  (center[l][j] - data[i][j])**2 + s
	            dist[i][l] = s**0.5
	    return dist
	
	#根据计算得到的距离对样本分类
	def Cluter(dist):
	    num = np.array(dist)
	    hang = num.shape[0] #数据个数
	    k = num.shape[1] #中心个数
	    clu = [[] for row in range(k)] #初始化存放样本标号的距离
	    for i in range(hang):
	        temp = dist[i][:].index(min(dist[i][:]))#求样本距离哪个聚类中心最近
	        for j in range(k):#判断样本属于哪个距离中心
	            if temp == j:
	                clu[j].append(i)
	                break
	    return clu
	            
	#重新计算聚类中心

	def clu_center(clu,data,k):
	    for i in range(k):
	        for j in range(2):#(np.array(data).shape[1]):
	            temp = 0
	            for k in range(len(clu[i])):#len(clu[i])为每个聚类中心样本的个数
	                temp = data[clu[i][k]][j] + temp
	            center[i][j] = temp / len(clu[i]) #重新计算得到的聚类中心
	    return center
	
	if __name__ == "__main__":
	    k =3#聚类中心个数
	    data = [[0,0],[3,8],[2,2],[1,1],[5,3],[4,8],[6,3],[5,4],[6,4],[7,5]]
	    #center = random.sample(data,k)#随机选取初始聚类中心
	    center = [[0,0],[3,8],[2,2]]#固定初始聚类中心
	    dist = distEC(data,center,k)#计算样本与聚类中心的距离
	    clu = Cluter(dist)#聚类
	    center1 = clu_center(clu,data,k)#重新计算聚类中心
	    center2 = []
	    while center1 != center2: #当聚类中心相同时，停止聚类
	        dist = distEC(data,center1,k)
	        clu = Cluter(dist)
	        center2 = center1
	        center1 = clu_center(clu,data,k)
	        print(center1,center2)
	
	    #画图
	    plt.figure()
	    for j in range(10):
	        plt.plot(data[j][0],data[j][1],'b.')
	    for i in range(k):
	        plt.plot(center1[i][0],center[i][1],'ro')
	    plt.xlabel('x')
	    plt.ylabel('y')
	    plt.show()
        

        
### 2.5.2 ISODATA算法（迭代自组织的数据分析算法）

K—均值算法比较简单，但它的自我调整能力也比较差。这主要表现在类别数不能改变，受代表点初始选择的影响也比较大。

ISODATA算法的功能与K—均值算法相比，在下列几方面有改进：

- 可以改变类别数目。通过类别的合并与分裂来实现。
- 合并主要发生在某一类内样本个数太少的情况，或两类聚类中心之间距离太小的情况。为此设有最小类内样本数限制，以及类间中心距离参数。
- 分裂则主要发生在某一类别的某分量出现类内方差过大的现象，因而宜分裂成两个类别，以维持合理的类内方差。给出一个对类内分量方差的限制参数，用以决定是否需要将某一类分裂成两类。
- 由于算法有自我调整的能力，因而需要设置若干个控制用参数。，如聚类数期望值K、每次迭代允许合并的最大聚类对数L、及允许迭代次数I等。


## 2.6 聚类结果评价

可考虑用以下几个指标来评价聚类效果：

- 聚类中心之间的距离：距离值大，通常可考虑分为不同类
- 聚类域中的样本数目：样本数目少且聚类中心距离远，可考虑是否为噪声
- 聚类域内样本的距离方差： 方差过大的样本可考虑是否属于这一类


    
    
