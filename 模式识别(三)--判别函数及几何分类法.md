---
title: 模式识别(三)--判别函数及几何分类法
date: 2018-11-8 19:02:50
tags:  模式识别
categories: "模式识别"
mathjax: true

---

统计模式识别分为聚类分析和判别函数法两大类，聚类分析属于非监督分类，判别函数法属于监督分类。判别函数法需要有足够的先验知识，首先利用已知类别的训练样本集确定判别函数，然后利用训练好的判别函数对未知的模式进行识别分类<!--more-->

判别函数，又可以分为线性判别函数法，非线性判别函数和统计决策方法。其中线性判别函数和非线性判别函数法是几何分类法，用户研究确定性事件的识别非零。统计决策方法是概率分类法，用于研究随机事件的识别分类。

![](https://i.imgur.com/45CWBgs.png)

## 3.1 判别函数

模式识别系统的主要作用是判别各个模式的所在类别。

若分属于ω1，ω2的两类模式可用一方程d(X)=0来划分，那么称d(X)为判别函数或称判决函数、决策函数。

例：一个二维的两类判别问题,模式分布如图示，这些分属于ω1，ω2两类的模式可用一直线方程d(X)=0来划分。

![](https://i.imgur.com/aufZhLj.png)

$d(X)=w_1x_1+w_2x_2+w_3=0$

式中：$x_1,x_2$为坐标向量，$w_1,w_2,w_3$为方程参数。

- 如果$d(X)>0$，则$X \in w_1$类；
- 如果$d(X)<0$，则$X \in w_2$类；
- 如果$d(X)=0$，则$X \in w_1$或$X \in w_2$类或拒绝。

维数=3是：判别边界是一平面。维数>3时，判别边界为一超平面。


## 3.2 线性判别函数

如果一些模式类能用线性判别函数分开，则称这些模式类是线性可分的。

### 3.2.1 线性判别函数的一般形式

将二维模式推广到n维，线性判别函数的一般形式为：

$d(X) = w_1x_1+w_2x_2+\dots+w_nx_n+w_{n+1}=W_0^TX+w_{n+1}$

式中：$X=[x_1,x_2,\dots,x_n]^T$，$W_0=[w_1,w_2,\dots,w_n]^T$；$W_0$是权向量，即参数向量。

增广向量形式：

$d(X)=w_1x_1+w_2x_2+\dots+w_nx_n+w_{n+1} \cdot 1 = [w_1,w_2,\dots,w_n, w_{n+1}][x_1,x_2,\dots,x_n,1]^T = W^TX$

式中：$X=[x_1,x_2,\dots,x_n,1]^T$，$W_0=[w_1,w_2,\dots,w_n,w_{n+1}]^T$为增广权向量。

### 3.2.1 线性判别函数的性质

#### 1.两类情况

若已知两类模式$w_1,w_2$，则判别函数$d(X)=W^TX$，具有如下性质：

![](https://i.imgur.com/geD5PCG.png)

$d(x)=0$是不可分情况。

#### 2.多类情况

对$M$个线性可分模式类，$w_1,w_2,\dots,w_M$，有三种划分方式：

- $w_i/\bar{w_i}$两分法
- $w_i/w_j$两分法
- $w_i/w_j$两分法特例

##### $w_i/\bar{w_i}$两分法

用线性判别函数将属于$w_i$类的模式与其余不属于$w_i$类的模式分开。

![](https://i.imgur.com/OrGRvyT.png)

将某个待分类模式$X$分别代入$M$个类的$d(X)$中，若只有$d_i(X)>0$，其他$d(X)$均$<0$，则判为$ω_i$类。

对某一模式区，$d_i(X)>0$的条件超过一个，或全部的$d_i(X)<0$，分类失效。相当于不确定区(indefinite region ，IR)。

![](https://i.imgur.com/r5NdfVC.png)

##### $w_i/w_j$两分法

一个判别界只能分开两个类别，不能把其余所有的类别都分开。能够分开$w_i$类和$w_j$类的判别函数为：$d_{ij}(X)=W_{ij}^TX$，这里$d_{ij}=-d_{ji}$。

判别函数性质：

$d_{ij}(X)>0, \forall j \neq i, i,j = 1,2,\dots, M$，若$X \in w_i$

在$M$类模式中，与$i$有关的$M-1$个判决函数全为正时，$X \in ωi$。其中若有一个为负，则为IR区。

$w_i/w_j$两分法每分离出一个模式，需要$M-1$个判别函数，而要分离所有的$M$类模式，共需要$M(M-1)/2$个判别函数。


#### $w_i/w_j$两分法特例

当$w_i/w_j$两分法中的判别函数$d_{ij}(X)$，可以分解为：$d_{ij}(X)=d_i(X)-d_j(X)$时，那么$d_i(X) > d_j(X)$相当于多累情况2中的$d_{ij}(X)>0$。


因此，具有判别函数：$d_i(X)=W^T_iX, i=1,\dots,M$的$M$类情况，判别函数性质为：

$d_i(X) > d_j(X)  \forall j \neq i, i,j = 1,2,\dots, M$ 若$X \in w_i$

或 $d_i(X) \max{\lbrace d_k(X), k= 1,2,\dots, M \rbrace} $ 若$X \in w_i$


## 3.3 广义线性判别函数

对非线性边界：通过某映射，把模式空间X变成$X^{\ast}$，以便将X空间中非线性可分的模式集，变成在$X^{\ast}$空间中线性可分的模式集。

### 1. 非线性多项式函数

非线性判别函数的形式之一是非线性多项式函数。 设一训练用模式集$\lbrace x \rbrace$，在模式空间$X$中线性不可分，非线性判别函数形式如下：

$d(X)=w_1f_1(X)+w_2f_2(X)+\dots+w_kf_k(X)+w_{+1} = \sum_{i=1}^{k+1}w_if_i(X)$

其中$\lbrace f_i(X),i=1,2,\dots,k \rbrace$是模式$X$的单值函数，$f_{k+1}(X)=1$。

广义形式的模式向量为：

$X^{\ast}=[x_1^{\ast}.x_2^{\ast},\dots,x_k^{\ast},1]^T=[f_1(X),f_2(X),\dots,f_k(X),1]^T$

这里$X^{\ast}$空间的维数$k$高于$X$空间的维数$n$。$d(X)$可以改写为：

$d(X) = W^TX^{\ast}=d(X^{\ast}), W=[w_1,w_2,\dots,w_k,w_{+1}]^T$

上式是线性的。讨论线性判别函数并不会失去一般性的意义。

**问题**

非线性变换可能非常复杂 。

维数大大增加： 维数灾难。 

![](https://i.imgur.com/Tlka5WK.png)

## 3.4 线性判别函数的几何性质

略

## 3.5 感知器算法

对于线性判别函数，当模式的维数已知时判别函数的形式实际上就已经定了下来，如：

二维: $X=(x_1,x_2)^T, d(X)=w_1x_1+w_2x_2+w_3$

三维： $X=(x_1,x_2，x_3)^T, d(X)=w_1x_1+w_2x_2+w_3x_3+w_4$

剩下的工作就是确定权重向量，只要求出权重向量，分类器的设计即告成功。

### 算法描述

![](https://i.imgur.com/HdQnVrQ.png)


### 收敛性

如果经过算法的有限次迭代运算后，求除了一个使训练集中所有样本都能正确分离的$W$,则称算法是收敛的。可以证明感知器算法是收敛的。对于感知器算法，只要模式是线性可分的，就可以在有限的迭代步数内求出权重向量的解。

## 3.6 梯度法


## 3.7 最小平方误差算法


感知器算法、梯度算法和固定增量算法或其他类似算法，只有当被分类模式可分离是才收敛，在不可分的情况下，算法会来回摆动，始终不收敛。另一方面，在训练样本较多的情况下，即使模式类线性可分的，也不可能事先算出达到收敛时所需的迭代步数。很难判断操作不收敛的原因是由于迭代过程收敛的缓慢还是由于模式本身就线性不可分。

最小平方误差（LMSE）算法的推导利用了梯度的概念，他除了对线性可分的模式类收敛，对线性不可分的情况也可以在算法的迭代过程中明确地指示出来。

### 1. 分类器的不等式方程

两类分类问题的解相当于求一组线性不等式的解。如果给出分别属于$w_1,w_2$两个模式的训练样本集$\lbrace X_i,i=1,2,\dots,N\rbrace$，就可以求出权重向量$W$，它们的性质满足：

$W^TX_i > 0$

则上式可以分开写为：

![](https://i.imgur.com/OMAhPvH.png)


感知器算法就是通过解不等式组求得的W，如果模式是线性可分的，则X的每一个$(n+1) \times (n+1)$阶子矩阵的都等于$(n+1)$。

### 2. LMSE算法

#### 原理

LMSE算法把满足$XW>0$的求解，改为满足：

$XW=B$

的求解，式中$B=(b_1,b_2,\dots,b_n)^T$是各分离均为正值的矢量。因此该式于$XW>0$是等价的。

在方程组中，方程的个数多余未知数个数时，也就是行数大于列数时，通常没有精确解存在，称为<b>矛盾方程组，一般求解近似解。

在模式识别中，训练样本数$N$总是大于模式的维数$n$，因此上式中方程的个数大于未知数$W$分量的个数，是矛盾方程组，只能求近似解，方法是求满足：

$||XW^{\ast}-B|| = 极小$

的$W^{\ast}$，$W^{\ast}$称为最小二乘近似解，也称为最优解。LMSE算法的出发点就是选择一个准则函数$J$，使得准则函数最小，$XW=B$可以得到最小二乘近似解。依据这种思路，可将LMSE算法的准则函数定义为:

$J(W,X,B)=\frac{1}{2}||XW-B||^2$


![](https://i.imgur.com/x6Nlep4.png)

准则函数可继续写成：

$J(W,X,B)=\frac{1}{2}||XW-B||^2=\frac{1}{2}\sum_{i=1}^N(W^TX_i-b_i)^2$

准则函数的值等于$W^TX_i$与$b_i$误差的平方之和，我们的目标是使这个误差的平方的最小化，由此称之为最小平方误差算法。

#### 算法描述

1、 将$N$个分属于$w_1$和$w_2$类的$n$维模式样本写成增广形式，将属于$w_2$的训练样本乘以-1，写出规范化增广样本矩阵$X$。

2、 求$X$得伪逆矩阵$X^{\ast}=(X^TX)^{-1}X^T$。

3、 设置初值$c$和$B(1)$，$c$为正得校验增量，$B(1)$的各分量大于零，括号中数字代表迭代次数$k=1$，开始迭代：

计算$W(1)=X^{\ast}B(1)$....

4、 计算$e(k)=XW(k)-B(k)$，进行可分性判别。

如果$e(k)=0$，模式类线性可分，解为$W(k)$，算法结束。

如果$e(k)>0$，模式类线性可分，有界。若进入第5步继续迭代，可是$e(k) \rightarrow 0$，得到最优解。

如果$e(k)<0$，停止迭代，检查$XW(k)$，若$XW(k)>0$，有解；否则无解，算法结束。

若不是上述任一种情况，说明$e(k)$的各分量值有正有负，进入第5步：

5、 计算$W(k+1)$和$B(k+1)$。

方法1：分别计算$W(k+1)=W(k)+cX^{\ast}$和$B(k+1)=B(k)+c[e(k)+|e(k)|]$

方法2：先计算$B(k+1)=B(k)+c[e(k)+|e(k)|]$，再计算$W(k+1)=X^{\ast}B(k=1)$。

迭代次数$k$加1，返回第4步。