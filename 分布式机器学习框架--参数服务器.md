---

title:  分布式机器学习框架--参数服务器
date: 2018-10-10 15:30:14
tags: 分布式机器学习

---

在大规模数据上跑机器学习任务是过去十多年内系统架构师面临的主要挑战之一，许多模型和抽象先后用于这一任务。<!--more-->


参数服务器的概念最早来自Alex Smola于2010年提出的并行LDA的框架[4]。它通过采用一个分布式的Memcached作为存放参数的存储，这样就提供了有效的机制用于在分布式系统不同的Worker节点之间同步模型参数，而每个Worker只需要保存它计算时所依赖的一小部分参数即可。当然，这里存放参数的存储跟做OLTP应用中的Key-Value抽象有所不同，因为以Key-Value为单元进行频繁的参数数据交互会导致过高的通信开销，因此参数服务器通常采用数学封装来进行参数同步，比如向量，张量，矩阵的行列等。


上图的sampler是并行LDA里的组件，可类比为通用参数服务器框架里的计算单元。Smola提出的模型是最早的参数服务器抽象，随后出现了许多改进，最出名的应当是Google的跨界高人Jeff Dean 2012年进一步提出了第一代Google大脑的解决方案DistBelief[5]，主要用于超大规模深度学习网络的训练。DistBelief将巨大的深度学习模型分布存储在全局的参数服务器中，计算节点通过参数服务器进行信息传递，很好地解决了SGD和L-BFGS算法的分布式训练问题。由于SGD和L-BFGS是机器学习的普遍性优化问题，因此尽管DistBelief是作为深度学习的系统框架而提出，但DistBelief的核心结构却可以应用到多种普通机器学习手段中。相比最早的参数服务器模型，DistBelief把该模型扩展成为更加通用和灵活的框架，豆瓣的Paracel[3]正是参考DistBelief的直接实现，先来看看Paracel和DistBelief模型：

![](https://i.imgur.com/wxlQ9bu.png)

图中是分布式异步SGD架构流程图，运行时，需要把训练数据分为多个子集，然后在每个子集上运行模型的多个副本，模型通过集中式的参数服务器通信，参数服务器存放了模型的全部参数和状态。异步体现在两方面：模型的副本独立运行；参数服务器的分片也各自独立运行。DistBelief没有过多谈论系统实现，从Paracel里我们可以看到具体的工程实现：总体上Paracel实现非常简单，参数服务器直接采用内存hashtable，并封装了对分网络，图，稀疏矩阵，稠密矩阵等数据格式用于参数同步。Paracel解决的另一问题是straggler问题：由于一些软硬件的原因，节点的计算能力往往不尽相同。对于迭代问题来说，每一轮结束时算得快的节点都需等待算得慢的节点算完，再进行下一轮迭代。这种等待在节点数增多时将变得尤为明显，从而拖慢整体的性能。Paracel放宽了“每个迭代步都等待”这个约束：当在一轮迭代结束时，算得快的节点可以继续下一轮迭代，但不能比最慢的节点领先参数s个迭代步。当领先超过s个迭代步，Paracel才会强制进行等待。这样异步的控制方式既从整体上省去了等待时间，也能间接地帮助慢的节点赶上。从优化问题的角度来看，虽然单迭代步收敛得慢了，然而每个迭代步的时间开销变少了，总体上收敛也就变快了。这种做法又叫Staleness Synchronous Parallel (SSP)，基本思想是允许各机器以不同步调对模型进行更新，但是加一个限制，使得最快的机器的进度和最慢机器的进度之差不要太大。这样做的好处是：既减轻慢的机器拖整个系统的后腿，又能保证模型的最终收敛。


SSP是由CMU Eric Xing的Petuum项目组提出的[6]，Paracel引入SSP使得豆瓣的参数服务器方案工程上更加成熟，在Paracel内部，SSP的等待通过调用MPI来实现。关于一致性收敛和Petuum，在下边还会有介绍。关于参数服务器，另一个重要的方面是容错设计。在几十台机器的集群上运行，这也许并不是一个问题，但是如果在有上千台机器的集群上运行任务，节点发生任务失败的概率就会大很多，如果缺乏容错设计，就会导致任务重启，从而浪费大量时间。不过，在Paracel的代码里并没有找到相关的处理逻辑，通常容错处理需要借助于Checkpoint来做快照，这样任务重启时无需从头进行，比如DistBelief就是这样处理。跟豆瓣的工程师咨询后已经确认，在开源版本的Paracel里确实还没有相关设计。


上面讲述了不少参数服务器的背景和系统结构，那么为什么参数服务器能够具备更好的性能呢？仍以SGD为例说明：在传统同步SGD中，如果一台机器失效，整个训练过程将会延时；但是对于异步SGD来讲，如果某个模型副本的一台机器失效，其他模型副本仍然继续处理样本并更新参数服务器中的模型参数，因此异步SGD具备更好的鲁棒性。此外，多种异步处理形式给最优化过程带来进一步的随机性：模型实例最可能是使用一个稍微过时的参数来计算梯度，因为这时其他的副本可能已经更新了参数服务器上的参数。除此之外还有其他随机的来源，因为参数服务器组的每台机器是行为独立的，所以无法保证在给定时间点上，每个节点的参数被更新的次数相同，或者以同样的顺序被更新。更进一步，因为模型副本使用不同的线程来获取参数和推送梯度值，故在同一时间戳上，单个副本内的参数将有额外的稍微不一致的现象。尽管对于非凸问题的这些操作的安全性缺乏理论基础，但是在实践中，这种放松一致性要求的做法是相当有效的。传统同步SGD的最优化过程，每次迭代选取的方向是由全部训练数据决定，或者由随机选定的一小部分训练集指定(mini-batch)。而异步的做法由于上述更多的随机性则会同时在很多方向上由不同的mini-batch选定不同梯度方向，这就好比整个最优化过程是以一个区域为单位进行的，而区域内的点代表不同SGD的过程，因此这种并行化的工作会带来性能上的提升。


介绍另外一个重要项目，来自Alex Smola的高徒——李沐设计的参数服务器[7]。这个项目在早期拥有一个独立域名http://parameterserver.org，后来因为李沐和陈天奇等国内英才成立的DMLC深度学习项目组，之前的项目也进行了重构因此转移到[7]所在的地址，而项目的背景介绍则在[8]和[9]。从架构上来说，ps-lite跟Paracel并没有什么不同，作为参数服务器，都需要一个全局分布式的key-value用来存储算法的模型或参数。当计算节点需要某个参数的时候，可以从参数服务器上读取。用户可定义不同的函数在参数服务器端对参数进行更新、过滤等操作。在大部分情况下，计算节点之间的通信都是通过参数服务器进行。


ps-lite应当属于第三代参数服务器，就是提供了更加通用的设计，在架构上包含一个Server Group和若干个Worker Group：

Server Group用来做参数服务器，每个Server Node存放一个参数分片，由Server Manager管理整个Server Group，维持整个Server Group的元数据的一致性视图，以及参数分片情况。

每个Worker Group运行一个应用，Worker Node只跟Server Node通信用来更新参数，Worker Node之间没有任何交互。每个Worker Group内有一个调度器，负责给Worker Nodes分配任务以及监控，如果有Worker Node挂掉或者新加入，调度器负责重新调度剩余的任务。


![](https://i.imgur.com/FiWlS8B.png)

跟Paracel不同，ps-lite提供了多种数据一致性选择：

Sequential：这种情况下，所有任务顺序进行，因此数据严格一致，不会出现不同副本看到的数据有不同的情况，因此实际上跟前文介绍的BSP是等价的。

Eventual：这种情况下，所有任务并行执行，因此拥有最大的随机性。Eventual只适用于对于数据一致没有要求，非常健壮的算法，比如SGD。

Bounded Delay：每个任务需要设置最大超时时间，在该时间之前如果有任务未结束，那么新任务将会等待。Bounded Delay类似于上面的SSP，只不过这是用时间而SSP则是用迭代次数。


转自：[http://chuansong.me/n/2161528](http://chuansong.me/n/2161528)